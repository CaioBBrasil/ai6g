{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install and import all the Needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow keras keras-tuner numpy wget\n",
    "import wget h5py keras_tuner os\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Conv2D,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First choose the dataset that will be used on the next steps\n",
    "\n",
    "Datasets available:\n",
    "1. 64x16 Single Pilot 80_Ntrain -5_SNR: \n",
    "2. 8x4 Single Pilot 80_Ntrain -5_SNR: \n",
    "3. 8x4 Single Pilot 80_Ntrain 0_SNR: \n",
    "\n",
    "To download using the notebook change CHOSEN_DATASET on the next script to the desired dataset number(Example: \"CHOSEN_DATASET = 1\" to get the 64x16 Single Pilot 80_Ntrain -5_SNR dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_DATASET = 2\n",
    "BASE_LINK=\"https://nextcloud.lasseufpa.org/remote.php/webdav/Lasse/Projects/UFA27_SmartHaul/MIMO-Hybrid-Channel-Estimation/Datasets/\"\n",
    "LINKS={\n",
    "    1:\"Pilots_SNR_(-5dB)_64x16(80pilots)_single_sub.hdf5\",\n",
    "    2:\"Pilots_SNR_(-5dB)_8x4(80pilots)_single_sub.hdf5\",\n",
    "    3:\"Pilots_SNR_(0dB)_8x4(80pilots)_single_sub.hdf5\",\n",
    "    \"channels_64x16_single_sub\":\"Channels_64x16_single_sub.hdf5\",\n",
    "    \"channels_8x4_single_sub\":\"Channels_8x4_single_sub.hdf5\",\n",
    "}\n",
    "if not os.path.isfile(LINKS[CHOSEN_DATASET]):\n",
    "    wget.download(BASE_LINK + LINKS[CHOSEN_DATASET])\n",
    "if CHOSEN_DATASET > 1:\n",
    "    if not os.path.isfile(LINKS[\"Channels_8x4_single_sub\"]):\n",
    "        wget.download(BASE_LINK + LINKS[\"Channels_8x4_single_sub\"])\n",
    "        CHOSEN_CHANNELS = \"Channels_64x16_single_sub.hdf5\"\n",
    "elif not os.path.isfile(LINKS[\"channels_64x16_single_sub\"]):\n",
    "    wget.download(BASE_LINK + LINKS[\"Channels_64x16_single_sub\"])\n",
    "    CHOSEN_CHANNELS = \"Channels_8x4_single_sub.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following values according to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keras tuner parameters\n",
    "MAX_TRIALS = 200\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "\n",
    "\n",
    "# Change these values acording to the dataset\n",
    "NR = 8 \n",
    "NT = 4 \n",
    "LR = 4 \n",
    "CHANNELS_USED = 9000\n",
    "SUBCARIERS_USED_TRAIN = 1\n",
    "SUBCARIERS_USED_OUT = 1\n",
    "NC = 10000\n",
    "NTRAIN = 80\n",
    "CHANNELS_USED_TEST = NC - CHANNELS_USED\n",
    "\n",
    "\n",
    "\n",
    "# Directory of the Channels Matrix file\n",
    "CHANNELS_FILE = h5py.File(LINKS[CHOSEN_DATASET], \"r\")\n",
    "\n",
    "\n",
    "# Directory of the Pilots Matrix file\n",
    "PILOTS_FILE = h5py.File(CHOSEN_CHANNELS, \"r\")\n",
    "\n",
    "\n",
    "input_shape = (NTRAIN * LR, SUBCARIERS_USED_TRAIN)\n",
    "output_shape = (NR, NT, SUBCARIERS_USED_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used as a metric in keras to calculate the NMSE(Normalized Mean Squared Error) for each train or validation example that will be iterated in the training process, providing clear metrics to the whole process and enabling the use of early_stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the function to calculate NMSE for each batch\n",
    "def NMSEtrainComplex(y_true, y_pred):\n",
    "    sub = y_pred[:, :, :] - y_true[:, :, :]\n",
    "    H_k = y_true[:, :, :]\n",
    "    nmse = tf.norm(sub, ord=\"fro\", axis=(1, 2)) ** 2\n",
    "    den = tf.norm(H_k, ord=\"fro\", axis=(1, 2)) ** 2\n",
    "\n",
    "    result = (nmse / den)\n",
    "\n",
    "    return 10*tf.experimental.numpy.log10(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the NN model will be crated, a chain of Dense layers is being used with relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense((512), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(np.prod(output_shape), activation=\"linear\"))\n",
    "model.add(Reshape(output_shape))\n",
    "model.compile(\n",
    "    loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[NMSEtrainComplex]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_inputData = PILOTS_FILE[\"pilots\"][:NC, :, :SUBCARIERS_USED_TRAIN]\n",
    "training_outputData = CHANNELS_FILE[\"channels\"][:NC, :, :, :SUBCARIERS_USED_OUT]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "        training_inputData,\n",
    "        training_outputData,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_NMSEtrainComplex\",\n",
    "                min_delta=5e-3,\n",
    "                patience=40,\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                filepath=\"./model\",\n",
    "                monitor=\"val_NMSEtrainComplex\",\n",
    "                verbose=1,\n",
    "                save_best_only=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                factor=0.5,\n",
    "                min_delta=5e-2,\n",
    "                patience=20,\n",
    "                cooldown=5,\n",
    "                verbose=1,\n",
    "                min_lr=1e-7,\n",
    "            ),\n",
    "        ],\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c6faf7a6b2e5e76de197b2e43c7ba4b62616900eb00f6bb925ee744c17b34a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
