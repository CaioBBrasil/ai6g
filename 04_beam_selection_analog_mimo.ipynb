{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#   Copyright (c) 2022 LASSE\n",
    "#\n",
    "#   This program is free software; you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License version 3 as\n",
    "#   published by the Free Software Foundation;\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#\n",
    "#   You should have received a copy of the GNU General Public License\n",
    "#   along with this program; if not, write to the Free Software\n",
    "#   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\n",
    "################################################################################\n",
    "# Based on Public Domain code written by LASSE: Ailton Oliveira, Aldebaro Klautau, Arthur Nascimento, Diego Gomes, Jamelly Ferreira, João Borges, Luan Gonçalves, and Walter Frazão.\n",
    "\n",
    "\"\"\"Trains a deep NN for choosing top-K beams\n",
    "Adapted by AK: Aug 7, 2018\n",
    "See\n",
    "https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "and\n",
    "https://stackoverflow.com/questions/45642077/do-i-need-to-use-one-hot-encoding-if-my-output-variable-is-binary\n",
    "See for explanation about convnet and filters:\n",
    "https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d\n",
    "and\n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, \\\n",
    "    Flatten, MaxPooling2D, Dense, Input, Dropout, ReLU\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def createArchitecture(self,num_classes,input_shape):\n",
    "        '''\n",
    "        Returns a NN model.\n",
    "        numClasses: a scalar which denotes the number of classes to be predicted\n",
    "        input_shape: a tuple with the dimensions of the input of the model\n",
    "        '''\n",
    "        dropProb=0.3\n",
    "        input_lid = Input(shape = input_shape)\n",
    "                    \n",
    "        layer = Conv2D(10,kernel_size=(13,13),\n",
    "                            activation='relu',\n",
    "                            padding=\"SAME\",\n",
    "                            input_shape=input_shape)(input_lid)\n",
    "        layer = Conv2D(30, (11, 11), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = Conv2D(25, (9, 9), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = MaxPooling2D(pool_size=(2, 1))(layer)\n",
    "        layer = Dropout(dropProb)(layer)\n",
    "        layer = Conv2D(20, (7, 7), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = MaxPooling2D(pool_size=(1, 2))(layer)\n",
    "        layer = Conv2D(15, (5, 5), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = Dropout(dropProb)(layer)\n",
    "        layer = Conv2D(10, (3, 3), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = Conv2D(1, (1, 1), padding=\"SAME\", activation='relu')(layer)\n",
    "        layer = Flatten()(layer)\n",
    "        out = Dense(num_classes,activation='softmax')(layer)\n",
    "        \n",
    "        architecture = Model(inputs = input_lid, outputs = out)\n",
    "        \n",
    "        return architecture\n",
    "\n",
    "    def build_resblock(self, input_shape, nChannels, maxpooling=True):\n",
    "        input = Input(shape=input_shape)\n",
    "        x = Conv2D(nChannels, (3, 3), padding=\"same\", activation=\"relu\")(input)\n",
    "        x = Conv2D(nChannels, (3, 3), padding=\"same\", activation=None)(x)\n",
    "        x = input + x\n",
    "        x = ReLU()(x)\n",
    "        if maxpooling:\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")(x)\n",
    "        return Model(inputs=input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Support functions\n",
    "###############################################################################\n",
    "\n",
    "# For description about top-k, including the explanation on how they treat ties (which can be misleading\n",
    "# if your classifier is outputting a lot of ties (e.g. all 0's will lead to high top-k)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k\n",
    "\n",
    "\n",
    "def top_10_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=10)\n",
    "\n",
    "\n",
    "def top_30_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=30)\n",
    "\n",
    "\n",
    "def top_50_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=50)\n",
    "\n",
    "\n",
    "def top_100_accuracy(y_true, y_pred):\n",
    "    return metrics.top_k_categorical_accuracy(y_true, y_pred, k=100)\n",
    "\n",
    "\n",
    "def sub2ind(array_shape, rows, cols):\n",
    "    ind = rows * array_shape[1] + cols\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0] * array_shape[1]] = -1\n",
    "    return ind\n",
    "\n",
    "\n",
    "def ind2sub(array_shape, ind):\n",
    "    ind[ind < 0] = -1\n",
    "    ind[ind >= array_shape[0] * array_shape[1]] = -1\n",
    "    rows = ind.astype(\"int\") / array_shape[1]\n",
    "    cols = ind % array_shape[1]\n",
    "    return (rows, cols)\n",
    "\n",
    "\n",
    "def beamsLogScale(y, thresholdBelowMax):\n",
    "    y_shape = y.shape\n",
    "\n",
    "    for i in range(0, y_shape[0]):\n",
    "        thisOutputs = y[i, :]\n",
    "        logOut = 20 * np.log10(thisOutputs + 1e-30)\n",
    "        minValue = np.amax(logOut) - thresholdBelowMax\n",
    "        zeroedValueIndices = logOut < minValue\n",
    "        thisOutputs[zeroedValueIndices] = 0\n",
    "        thisOutputs = thisOutputs / sum(thisOutputs)\n",
    "        y[i, :] = thisOutputs\n",
    "\n",
    "    return y\n",
    "\n",
    "#converts the combined channel gains into the label vector y\n",
    "#to be used for training the model. For instance, if a softmax\n",
    "#activation is used in the DNN output, then make y sum up to 1.\n",
    "def prepare_output_labels(original_gains):  \n",
    "    method = 1 #for using DNN output with softmax activation    \n",
    "    if method == 1:\n",
    "        thresholdBelowMax = 60 #threshold in dB for zeroing gains in log domain\n",
    "        y = log_normalize_channel_gain(original_gains,thresholdBelowMax)\n",
    "    return y\n",
    "\n",
    "#converts to log scale and make zero the values below the specified threshold\n",
    "def log_normalize_channel_gain(y,thresholdBelowMax):\n",
    "    num_gains = len(y)\n",
    "    #print('AK',num_gains)\n",
    "    for i in range(0,y.shape[0]):            \n",
    "            thisOutputs = y[i,:]\n",
    "            logOut = 20*np.log10(thisOutputs + 1e-30)\n",
    "            minValue = np.amax(logOut) - thresholdBelowMax\n",
    "            zeroedValueIndices = logOut < minValue\n",
    "            thisOutputs[zeroedValueIndices]=0\n",
    "            #normalize to sum up to 1, and enable using softmax activation\n",
    "            thisOutputs = thisOutputs / sum(thisOutputs)\n",
    "            y[i,:] = thisOutputs        \n",
    "    return y\n",
    "\n",
    "#instead of representing the pair of beam indices with two integers, use only one integer.\n",
    "#for instance, if yMatrix has a shape of (11194, 8, 32), where 8 is the number of Rx antennas\n",
    "#and 32 the number of Tx antennas, then y_output has the shape (11194, 256) because 256=8*32.\n",
    "#this way, a single index will indicate the pair of indices\n",
    "def convert_pair_to_index(yMatrix):\n",
    "    yMatrix = np.abs(yMatrix)\n",
    "    yMatrix /= np.max(yMatrix)\n",
    "    yMatrixShape = yMatrix.shape\n",
    "    num_classes = yMatrix.shape[1] * yMatrix.shape[2]\n",
    "    y = yMatrix.reshape(yMatrix.shape[0],num_classes)\n",
    "    return y, num_classes\n",
    "\n",
    "\n",
    "def getBeamOutput(output_file):\n",
    "\n",
    "    print(\"Reading dataset...\", output_file)\n",
    "    output_cache_file = np.load(output_file)\n",
    "    yMatrix = output_cache_file[\"output_classification\"]\n",
    "\n",
    "    num_classes = 256\n",
    "\n",
    "    pairs = [[i, j] for i in range(8) for j in range(32)]\n",
    "    yMatrix = yMatrix.tolist()\n",
    "    indexes = [pairs.index(yMatrix[item]) for item in range(len(yMatrix))]\n",
    "\n",
    "    y = to_categorical(indexes, num_classes)\n",
    "\n",
    "    return y, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './files_03_analog/lidar_input/lidar_input.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     13\u001b[0m lidar_train_input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./files_03_analog/lidar_input/lidar_input.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m lidar_train_cache_file \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_train_input_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m X_lidar \u001b[38;5;241m=\u001b[39m lidar_train_cache_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m X_lidar_train, X_lidar_validation \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     17\u001b[0m     X_lidar, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ai6g\\lib\\site-packages\\numpy\\lib\\npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './files_03_analog/lidar_input/lidar_input.npz'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Data configuration\n",
    "###############################################################################\n",
    "tf.device(\"/device:GPU:0\")\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "tgtRec = 3\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "lidar_train_input_file = \"./files_03_analog/lidar_input/lidar_input.npz\"\n",
    "lidar_train_cache_file = np.load(lidar_train_input_file)\n",
    "X_lidar = lidar_train_cache_file[\"input\"]\n",
    "X_lidar_train, X_lidar_validation = train_test_split(\n",
    "    X_lidar, test_size=0.2, random_state=seed, shuffle=True\n",
    ")\n",
    "print(\"Reading dataset... \", lidar_train_input_file)\n",
    "lidar_train_input_shape = X_lidar_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Output configuration\n",
    "output_file = \"./files_03_analog/beam_output/beams_output.npz\"\n",
    "\n",
    "print(\"Reading dataset...\", output_file)\n",
    "output_cache_file = np.load(output_file)\n",
    "yMatrix = output_cache_file['output_classification']\n",
    "y_output, num_classes = convert_pair_to_index(yMatrix)\n",
    "y = prepare_output_labels(y_output)\n",
    "\n",
    "(\n",
    "    X_lidar_train,\n",
    "    X_lidar_validation,\n",
    "    y_train,\n",
    "    y_validation,\n",
    ") = train_test_split(\n",
    "    X_lidar, y_output, test_size=0.2, random_state=seed, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Model configuration\n",
    "##############################################################################\n",
    "\n",
    "modelHand = ModelHandler()\n",
    "opt = Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=True,\n",
    "    name=\"Adam\",\n",
    ")\n",
    "\n",
    "model = modelHand.createArchitecture(\n",
    "    num_classes,\n",
    "    [\n",
    "        lidar_train_input_shape[1],\n",
    "        lidar_train_input_shape[2],\n",
    "        lidar_train_input_shape[3],\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=opt,\n",
    "    metrics=[\n",
    "        metrics.categorical_accuracy,\n",
    "        metrics.top_k_categorical_accuracy,\n",
    "        top_10_accuracy,\n",
    "        top_30_accuracy,\n",
    "        top_50_accuracy,\n",
    "        top_100_accuracy,\n",
    "    ],\n",
    ")\n",
    "model.summary()\n",
    "hist = model.fit(\n",
    "    X_lidar_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_lidar_validation, y_validation),\n",
    ")\n",
    "\n",
    "# with open(\"history.txt\", \"w\") as f:\n",
    "#     f.write(str(hist.history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##Baselines cores\"\"\"\n",
    "\n",
    "beam_weights = {}\n",
    "for i in range(y_train.shape[1]):\n",
    "    beam_weights[i] = 0\n",
    "\n",
    "for i in range(y_train.shape[0]):\n",
    "    scene_array = y_train[i, :]\n",
    "    beam_weights[np.argmax(scene_array)] += 1\n",
    "\n",
    "ocurrence = np.zeros((1, y_validation.shape[1]))\n",
    "oc_factor = sum(beam_weights.values())\n",
    "for b_index in beam_weights.keys():\n",
    "    ocurrence[0, b_index] = beam_weights[b_index] / oc_factor\n",
    "\n",
    "# ocurrence\n",
    "ocurrence_input = np.repeat(ocurrence, y_validation.shape[0], axis=0)\n",
    "ocurrence_output = softmax(ocurrence_input, axis=1)\n",
    "# rand\n",
    "rand_input = np.random.rand(X_lidar_validation.shape[0], 256)\n",
    "rand_output = softmax(rand_input, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#Accuracy / epochs\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(\n",
    "    hist.history[\"categorical_accuracy\"], \"b\", label=\"Categorical accuracy\", linewidth=2\n",
    ")\n",
    "ax.plot(hist.history[\"top_k_categorical_accuracy\"], \"k--\", label=\"Top 5\", linewidth=2)\n",
    "ax.plot(hist.history[\"top_10_accuracy\"], \"m\", label=\"Top 10\", linewidth=2)\n",
    "ax.plot(hist.history[\"top_30_accuracy\"], \"g--\", label=\"Top 30\", linewidth=2)\n",
    "ax.plot(hist.history[\"top_50_accuracy\"], \"r\", label=\"Top 50\", linewidth=2)\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Accuracy\")\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.title('LIDAR data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"##Creating Baselines\"\"\"\n",
    "\n",
    "predict_top_k = {}\n",
    "ocurrence_top_k = {}\n",
    "random_top_k = {}\n",
    "y_predict = model.predict(X_lidar_validation)\n",
    "for i in range(1, 51):\n",
    "    predict_top_k[f\"top_{i}\"] = (\n",
    "        np.sum(top_k_categorical_accuracy(y_validation, y_predict, k=i))\n",
    "        / y_predict.shape[0]\n",
    "    )\n",
    "    ocurrence_top_k[f\"top_{i}\"] = (\n",
    "        np.sum(top_k_categorical_accuracy(y_validation, ocurrence_output, k=i))\n",
    "        / y_predict.shape[0]\n",
    "    )\n",
    "    random_top_k[f\"top_{i}\"] = (\n",
    "        np.sum(top_k_categorical_accuracy(y_validation, rand_output, k=i))\n",
    "        / y_predict.shape[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## TOP 10 acurracy\"\"\"\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(predict_top_k[\"top_10\"])\n",
    "plt.hist(ocurrence_top_k[\"top_10\"])\n",
    "plt.hist(random_top_k[\"top_10\"])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.xlabel(\"Top k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(list(predict_top_k.values()), label=\"NN\")\n",
    "plt.plot(list(ocurrence_top_k.values()), label=\"Ocurrence\")\n",
    "plt.plot(list(random_top_k.values()), label=\"Dummy\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai6genv)",
   "language": "python",
   "name": "ai6g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9904cea129132a4266dc880d621b54b52a0f30f3f5fd705fff9762b6a3258347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
